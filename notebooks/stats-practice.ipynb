{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the data & explore\n\ndata = pd.read_csv('/kaggle/input/iris/Iris.csv')\n# data.head()\n# data.shape # 150,6\n# data.columns # Index(['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm','Species'], dtype='object')\n# print(data)\n# data.info()\n# data.describe()\n\n# print('Variability from greatest to smallest (measured by std): Petal length, Sepal length, Petal Width, Sepal Width')\n\n# EDA\n\n#### Q1 Which iris has the longest pedal length using average and median?\n## Answer: Virginica\n\n# data.groupby('Species').agg(['mean','median'])\n\n#### Q2 What does a scatter plot of the Sepal Length and Width look like compared to the petal length and width?\n\n## Method 1 ------------------\n\n# import matplotlib.pyplot as plt\n\n# fig, ax = plt.subplots(1,2)\n# for species in data['Species'].unique():\n#     ax[0].plot('SepalLengthCm','SepalWidthCm',data=data.loc[data['Species'] == species], linestyle='none', marker='.', label=species)\n#     ax[1].plot('PetalLengthCm','PetalWidthCm',data=data.loc[data['Species'] == species], linestyle='none', marker='.', label=species)\n# plt.legend()\n# plt.show()\n\n# Method 2 ------------------\n\n\n\n\n# Stats\n\n### Q1 Can I compute an ecdf of the petal lengths for the different species?\n\n## Method 1 ----------------------------------\n\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# def ecdf(data):\n#     \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n#     # Number of data points: n\n#     n = len(data)\n\n#     # x-data for the ECDF: x\n#     x = np.sort(data)\n\n#     # y-data for the ECDF: y\n#     y = np.arange(1, n + 1) / n\n\n#     return x, y\n\n# # print(data['Species'].unique()) # 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica'\n\n# x_setosa, y_setosa = ecdf(data.loc[data['Species'] == 'Iris-setosa']['PetalLengthCm'])\n# x_versicolor, y_versicolor = ecdf(data.loc[data['Species'] == 'Iris-versicolor']['PetalLengthCm'])\n# x_virginica, y_virginica = ecdf(data.loc[data['Species'] == 'Iris-virginica']['PetalLengthCm'])\n\n# # sns.set()\n# plt.style.use('ggplot')\n\n# fig, ax = plt.subplots()\n\n# ax.plot(x_setosa, y_setosa, linestyle='none', marker='.', color='blue', label='setosa')\n# ax.plot(x_versicolor, y_versicolor, linestyle='none', marker='.', color='green', label='versicolor')\n# ax.plot(x_virginica, y_virginica, linestyle='none', marker='.', color='red', label='virginica' )\n\n# ax.set(xlabel='petal length', ylabel='ECDF', title='Iris Petal Length')\n# ax.legend()\n\n## Method 2 ---------------------\n\n# from empiricaldist import Cdf \n\n# cdf_ = Cdf.from_seq(data.loc[data['Species'] == 'Iris-setosa']['PetalLengthCm'])\n# cdf_.plot()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Q1 What is does the distribution look like for a weight coin vs. a fair coin?\n## Hypothesis: I think the distribution will still be binomial, just shifted to the right\n## Answer: The null hypothesis was correct\n\n# \"\"\"I want to take 1000 samples with 1000 flips with a weighted coin having a probability of getting heads at 60%\"\"\"\n# import numpy as np\n# import matplotlib.pyplot as plt\n\n# iterations = 10000\n# flips = 10000\n# data = np.empty(iterations)\n\n# for i in range(iterations):\n#     successes = np.sum(.6 >= np.random.random(flips)) # count the number of heads below .6\n#     data[i] = successes\n    \n# plt.hist(data,histtype='step', density=True)\n# plt.show()\n\n## Q2 Can I make a similar plot using the numpy binomial function?\n## Answer: Yep\n\n# import numpy as np\n\n# iterations = 10000\n# flips = 10000\n# data = np.empty(iterations)\n\n# for i in range(iterations):\n#     successes = np.random.binomial(1000,.6) # count the number of heads below .6\n#     data[i] = successes\n    \n# plt.hist(data,histtype='step', density=True)\n# plt.show()\n\n### Q3 Binomial practice: What was the probability of the number of wins that the winner of fantasy football had?\n## Answer: .08724 and theoretically it was .08728 - so pretty close\n\n# So if each game you had a 50% chance of winning and then you ran that expiriment 13 times, what is the probability you win 9 times?\n\n# import numpy as np\n# import matplotlib.pyplot as plt\n\n# iterations = 100000\n# rounds = 13\n# data = np.empty(iterations)\n\n# # for i in range(iterations):\n# for i in range(iterations):\n#     successes = np.random.binomial(rounds, .5)\n#     data[i] = successes\n    \n# plt.hist(data,histtype='step', density=True)\n# plt.show()\n\n# print(np.sum(data == 9)/len(data))\n# print(.5**9) # Winning 9 times in a row\n\n# from math import factorial\n\n# factorial(13)/(factorial(13) * factorial(13-13))\n\n# import itertools\n# x = list(itertools.product(\"TF\", repeat=13))\n\n# count_wins = []\n# for i in x:\n#     count_wins.append(list(i).count('T'))\n\n# print(np.sum(np.array(count_wins) == 9)/len(count_wins))\n# len(count_wins)\n\n## Q4 Does having a rarer outcome determine if there was skill involved? e.g a 1 out of 13 chance of winning and they got 9 out of 13 wins. The 9 out of 13 wins was less \n## likely than the 1 out of 13 chance so there could be something there or it could also be chance. The only way to know is through repitition. This question is similar\n## to asking how to determine if the dice is weighted. Basically, if the probabilities don't converge to 50%. \n## I am using this reddit thread as an example\" https://www.reddit.com/r/statistics/comments/2ieyli/we_suspect_our_catan_dice_are_weighted_can/\n\n## Q4.A - I roll a dice 500 times and have a probability of a roll occurring 5/36 times. What does my 95% confidence interval look like?\n## Answer: So I think what is being said below is that it has to be more than 22 and less than 8 to be 95% confident the dice are weighted\n## It also looks like you could do bootstrap replicates if you only have one data set and then find the distribution and compute your 95% confidence interval from that\n## Additional resources: It looks like this is a thread where they say chi-squared would be more appropriate for determining weighted dice: https://www.reddit.com/r/statistics/comments/2ieyli/we_suspect_our_catan_dice_are_weighted_can/\n\n# Additional questions: \n\n## 1. What is the minimum number of rolls needed in order to be able to then bootstrap it?\n## 2. What is the formula for a binomial distribution so you don't have to use hacker statistics, but instead can mathematically find the confidence interval?\n\n# Method 1: Using a lot iterations -----------------------------\n\n# import numpy as np\n\n# iterations = 500\n# rolls = 100\n# data = np.empty(iterations)\n\n# for i in range(iterations):\n#     successes = np.random.binomial(rolls,5/36) # count the number of rolls < 5/36\n#     data[i] = successes\n\n# print(np.percentile(data, [2.5,97.5]))\n# plt.hist(data,histtype='step', density=True)\n# plt.show()\n\n# np.random.binomial(100,5/36)\n\n# Method 2 : One dataset and bootstrapping it ---------------------------------\n# Answer: less than 10 and more than 25, obviously this method is not as precise as above, but there is not that big of a difference\n\n# import numpy as np\n\n# rolls = 100\n# iterations = 500\n\n# np.random.seed(42)\n\n# successes = np.random.random(rolls) # represents 100 rolls\n\n# x = np.random.choice(successes,rolls) # represents the bootstrap\n\n# \"\"\"\n# 1. For each bootstrap, we want to know how many were 5/36 or below\n# 2. Then we take that dataset and plot it using the plt.hist function\n# 3. We take that same dataset and can find the confidence interval and compare it to the iterations we did up above\n# \"\"\"\n\n# data = np.empty(iterations)\n\n# for i in range(iterations):\n#     data[i] = np.sum(np.random.choice(successes,rolls) <= (5/36))\n    \n# plt.hist(data, density=True, histtype='step')\n# np.percentile(data, [2.5,97.5])\n\n\n\n# Khan Academy Practice ------------------------------------\n# Source: https://www.khanacademy.org/math/ap-statistics/random-variables-ap/binomial-mean-standard-deviation/v/finding-the-mean-and-standard-deviation-of-a-binomial-random-variable\n\n# import numpy as np\n\n# iterations = 500\n# rolls = 500\n# data = np.empty(iterations)\n\n# for i in range(iterations):\n#     successes = np.random.binomial(rolls,.02) # count the number of rolls < 5/36\n#     data[i] = successes\n\n# # print(np.percentile(data, [2.5,97.5]))\n# print(np.mean(data))\n# # plt.hist(data,histtype='step', density=True)\n# # plt.show()\n\n\n# Practice ------------------------------------\n## This is the theoretical distribution for a binomial function. It is simply the number of successes plotted out for a given combination.\n\n\n# import itertools\n# x = list(itertools.product(\"TF\", repeat=13))\n\n# # len(x) # 8192 \n\n# y = []\n# for i in x:\n#     y.append(np.sum(list(i).count('T')))\n# y.count(9) # 715\n\n\n# plt.hist(y, density=True)\n\n\n# np.mean(y) # 6.5\n# np.var(y) # 3.25\n# np.std(y) # 1.8\n\n## So getting back to the question is, is winning 9 out of 13 times significant? And since we can see our mean is 6.5, then the std is 6.5 * 1-p, which is 3.25.\n## So the variance is the number of successes * the percentage of failures. If it's 50% the variance adds back up to the mean. If it's more than 50%, \n## then the variance be less than the mean, which makes sense since we are more sure of a certain value being correct.So more variance means less surety of \n## success.\n\n## Anyway, so the std is 1.8 so 1.8 * 2 = 3.6 and so 6.5 + or - 3.6 results in something statistically significiant. So 2.9 to 10.1 is the 95% conf. interval. \n## So you could say that the results of the fantasy football game were expected and due to chance since nobody broke the significance threshold. \n## But then again, you will be right 95% of the time with this assumption and wrong 5% of the time. To increase how sure you are, you would have to do more\n## games and then see if the average is close to 6.5 games won. If it is not, then it's probably not random chance. So can I take the results that I had and then do a\n## bootstrap approach to see if the difference of the means was significant?\n\n\n## But this leads to another question. Does having more than 1 person get 9 mean it's significant? -> This relates to finding the difference in the distributions\n## How does the bootstrapping relate to my question? I know how to find the 95% confidence interval for a single value of a distribution, but what about my own distribution\n## and determining if the distribution is significantly different?\n\n## The answer to this seems to be if the only overlapping parts are outside the 95% conf interval, then that definitely would be considered significant. \n## This also means if both distributions are based on the same number of possibilities, then the probabilities of success are different, which shifts the\n## curve. \n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = [9,9,8,9,6,7,6,7,5,7,4,6,5,3]\n\nplt.hist(x, density=True, histtype='step')\nplt.show()\n\nnp.mean(x)\nnp.var(x)","execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"7"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Practice - Fantasy Football Stats Question ------------------------------------\n# Method 1 ------------------------\n\"\"\" Use the one-sample t-test approach using hacker statistics to check if the distributions are similar or not\"\"\"\n\n# Step 1 - Create bootstrap replicates of the 1D data\n\nimport numpy as np\n\n# ff_data = [9,9,8,9,6,7,6,7,5,7,4,6,5,3]\nff_data =   [13,13,13,13,2,2,2,2,9,8,7,7]\n\n\nimport itertools\nx = list(itertools.product(\"TF\", repeat=13))\n\nff_theoretical = []\nfor i in x:\n    ff_theoretical.append(np.sum(list(i).count('T')))\n\ndef bootstrap_replicate_1d(data, func): \n    \"\"\"Generate bootstrap replicate of 1D data.\"\"\" \n    bs_sample = np.random.choice(data, len(data)) \n    return func(bs_sample) \n\nff_data_bootstrap = np.random.choice(ff_data,len(y))\n\ndef draw_bs_reps(data, func, size=1): \n    \"\"\"Draw bootstrap replicates.\"\"\" \n\n    # Initialize array of replicates: bs_replicates \n    bs_replicates = np.empty(size) \n\n    # Generate replicates \n    for i in range(size): \n        bs_replicates[i] = bootstrap_replicate_1d(data,func) \n\n    return bs_replicates ","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"empirical_diff_means = np.mean(ff_theoretical) - np.mean(ff_data_bootstrap)\n\n# Concatenate the data sets\nff_concat = np.concatenate((ff_data_bootstrap, ff_theoretical))\n\n# Compute mean of all forces: mean_force \nmean_ff = np.mean(ff_concat) \n\n# Generate shifted arrays \nff_data_bootstrap_shifted = ff_data_bootstrap - np.mean(ff_data_bootstrap) + mean_ff \nff_theoretical_shifted = ff_theoretical - np.mean(ff_theoretical) + mean_ff \n\n# Compute 10,000 bootstrap replicates from shifted arrays \nbs_replicates_a = draw_bs_reps(ff_data_bootstrap_shifted, np.mean, size=10000) \nbs_replicates_b = draw_bs_reps(ff_theoretical_shifted, np.mean, size=10000) \n\n# Get replicates of difference of means: bs_replicates \nbs_replicates = bs_replicates_b - bs_replicates_a\n\n# Compute and print p-value: p \np = np.sum(abs(bs_replicates) >= abs(empirical_diff_means)) / len(bs_replicates) \n\nprint('p-value ={:e}'.format(p)) \nprint('Diff of means', empirical_diff_means)","execution_count":41,"outputs":[{"output_type":"stream","text":"p-value = 0.0\nDiff of means -1.0679931640625\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import ttest_ind\nimport scipy\n\nttest_ind(ff_theoretical, ff_data_bootstrap) # p-value for this one is .96","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"Ttest_indResult(statistic=-19.81146934747771, pvalue=2.4264774050755806e-86)"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}